# New Documentation Sources - Scraping Guide

**Date:** December 1, 2025  
**Status:** Production Ready  
**Method:** Unified Crawler (scrape-docs.ts)  
**New Sources:** 4 (Docker, FastAPI, Vue.js, PostgreSQL)

---

## âœ… Production Approach

All documentation scraping uses **one unified crawler** (`scrape-docs.ts`) for:

- âœ… Consistent interface
- âœ… Auto-discovery of pages
- âœ… Easy maintenance (1 file for 15+ sources)
- âœ… Better coverage (finds all linked pages)

---

## ğŸ†• New Sources Added

| Source         | Ecosystem    | URL                  | Status   |
| -------------- | ------------ | -------------------- | -------- |
| **Docker**     | cloud_infra  | docs.docker.com      | âœ… Ready |
| **FastAPI**    | python       | fastapi.tiangolo.com | âœ… Ready |
| **Vue.js**     | frontend_web | vuejs.org            | âœ… Ready |
| **PostgreSQL** | database     | postgresql.org/docs  | âœ… Ready |

All integrated into `scrape-docs.ts` - no separate files needed!

---

## ğŸš€ How to Scrape (Production)

### Step 1: Run Unified Scraper

```bash
cd apps/api

# Scrape new documentation sources
pnpm scrape docker        # Auto-crawls ~120-150 pages
pnpm scrape fastapi       # Auto-crawls ~80-100 pages
pnpm scrape vue           # Auto-crawls ~90-120 pages
pnpm scrape postgresql    # Auto-crawls ~100-150 pages
```

**What happens:**

- Crawler starts from configured start URLs
- Auto-discovers pages via link following
- Respects URL patterns (only relevant pages)
- Generates chunks with smart filtering
- Saves to `data/<source>-chunks.json`

---

## ğŸ“Š After Scraping: Database Setup

### Step 2: Add to Database

```sql
-- Insert new sources into doc_sources table
INSERT INTO doc_sources (id, name, base_url, ecosystem_id, description) VALUES
('docker', 'Docker', 'https://docs.docker.com', 'cloud_infra',
 'Containerization platform documentation'),
('fastapi', 'FastAPI', 'https://fastapi.tiangolo.com', 'python',
 'Modern Python web framework for building APIs'),
('vue', 'Vue.js', 'https://vuejs.org', 'frontend_web',
 'Progressive JavaScript framework'),
('postgresql', 'PostgreSQL', 'https://www.postgresql.org/docs', 'database',
 'Advanced open source relational database');
```

### Step 3: Index to Qdrant

```bash
# After scraping succeeds, index each source
pnpm index docker
pnpm index fastapi
pnpm index vue
pnpm index postgresql
```

### Step 4: Verify & Test

```bash
# Test queries
docstalk ask "How to build Docker containers?"
# Should detect: cloud_infra ecosystem â†’ docker

docstalk ask "FastAPI async endpoints?"
# Should detect: python ecosystem â†’ fastapi

docstalk ask "Vue 3 composition API?"
# Should detect: frontend_web ecosystem â†’ vue

docstalk ask "PostgreSQL indexes?"
# Should detect: database ecosystem â†’ postgresql
```

---

## ğŸ“ˆ Expected Results

After scraping and indexing, you should have:

### Coverage Update:

**Before:**

- frontend_web: 3 sources
- js_backend: 2 sources
- systems: 2 sources
- python: 1 source â†’ **2 sources** (+ fastapi)
- database: 1 source â†’ **2 sources** (+ postgresql)
- styling: 1 source
- ai_ml: 1 source
- cloud_infra: 0 sources â†’ **1 source** (+ docker)

**After:**

- **Total Sources:** 12 â†’ **15** (+ 3 new)
- **Ecosystem Coverage:** 7/8 (87.5%) â†’ **8/8 (100%)** âœ…
- **Empty Ecosystems:** 1 â†’ **0** ğŸ‰

---

## ğŸ¯ Ecosystem Impact

### ğŸŸ¥ Cloud & Infrastructure (NEW!)

**Before:** Empty âŒ  
**After:** docker âœ…

**Queries now supported:**

- "How to build Docker images?"
- "Docker compose configuration"
- "Container networking"

### ğŸŸ§ Python (Enhanced)

**Before:** python only  
**After:** python + fastapi âœ…

**Queries now supported:**

- "FastAPI async endpoints"
- "Python REST API with FastAPI"
- "Dependency injection in FastAPI"

### ğŸŸ¦ Frontend Web (Enhanced)

**Before:** react, nextjs, typescript  
**After:** react, nextjs, typescript, vue âœ…

**Queries now supported:**

- "Vue 3 composition API"
- "Comparing React vs Vue"
- "Vue routing and state management"

### ğŸŸ« Database (Enhanced)

**Before:** prisma only  
**After:** prisma + postgresql âœ…

**Queries now supported:**

- "PostgreSQL indexes and performance"
- "SQL joins in PostgreSQL"
- "Prisma with PostgreSQL"

---

## ğŸ”„ Future Scrapers (Priority Queue)

### Priority 1 (High Impact):

- **LangChain** (ai_ml) - AI framework
- **Django** (python) - Python web framework
- **Kubernetes** (cloud_infra) - Container orchestration
- **MongoDB** (database) - NoSQL database

### Priority 2 (Medium Impact):

- **Svelte** (frontend_web) - Frontend framework
- **Fastify** (js_backend) - Fast Node.js framework
- **AWS** (cloud_infra) - Cloud platform
- **Redis** (database) - In-memory database

### Priority 3 (Nice to Have):

- **Chakra UI** (styling) - React component library
- **Shadcn UI** (styling) - Modern UI components
- **OpenAI API** (ai_ml) - LLM API docs
- **Supabase** (database) - Backend as a service

---

## ğŸ› ï¸ Adding New Sources

To add new documentation sources, update `scrape-docs.ts`:

### 1. Add Configuration

```typescript
// In apps/api/scripts/scrape/sources/scrape-docs.ts

const DOC_CONFIGS = {
  // ... existing sources ...

  // Add new source:
  langchain: {
    name: "LangChain",
    baseUrl: "https://docs.langchain.com",
    startUrls: ["https://docs.langchain.com/docs/get-started/introduction"],
    urlPattern: /^https:\/\/docs\.langchain\.com\/docs/,
    maxPages: 150,
  },
};
```

### 2. Configuration Options

```typescript
{
  name: "Display Name",          // Shown in logs
  baseUrl: "https://...",        // Base URL for relative links
  startUrls: ["https://..."],    // Where to start crawling
  urlPattern: /regex/,           // Which URLs to crawl
  maxPages: 150,                 // Max pages to scrape
}
```

### 3. Run Scraper

```bash
pnpm scrape langchain
```

### 4. Add to Database

```sql
INSERT INTO doc_sources (id, name, base_url, ecosystem_id, description) VALUES
('langchain', 'LangChain', 'https://docs.langchain.com', 'ai_ml',
 'Framework for building LLM applications');
```

### 5. Index

```bash
pnpm index langchain
```

---

## âœ… Checklist for Adding New Source

- [ ] Add config to `scrape-docs.ts` DOC_CONFIGS
- [ ] Test scraper: `pnpm scrape <source>`
- [ ] Verify chunks in `data/<source>-chunks.json`
- [ ] Insert into `doc_sources` table with correct ecosystem_id
- [ ] Run indexer: `pnpm index <source>`
- [ ] Verify in Qdrant dashboard
- [ ] Test queries via chat or CLI

---

## ğŸ“ Notes

**Scraping Best Practices:**

- âœ… Use rate limiting (1s delay between requests)
- âœ… Handle errors gracefully
- âœ… Extract only main content
- âœ… Remove navigation/footers
- âœ… Convert HTML to Markdown
- âœ… Include metadata (url, title, section)

**Legal Considerations:**

- âœ… Only scrape publicly available documentation
- âœ… Respect robots.txt
- âœ… Include source attribution
- âœ… Don't overload servers

---

**Ready to scrape?** Start with high-priority sources! ğŸš€
